
# Introduction

This is a post relating with an issue encoutered recently, in which
the model.fit and model.evalute gives very different performance
metrics even on the same dataset. Quite a few similar issues have been
posted online. I will briefly introduce the issue, its potential causes,
and possible solutions in the post.


# Reference

https://stackoverflow.com/questions/51123198/strange-behaviour-of-the-loss-function-in-keras-model-with-pretrained-convoluti/51124511#51124511


https://github.com/keras-team/keras/issues/6977

https://github.com/tensorflow/tensorflow/issues/36446

https://github.com/keras-team/keras/pull/9965

https://github.com/keras-team/keras/issues/10014

https://github.com/keras-team/keras/issues/10045

https://github.com/tensorflow/tensorflow/issues/36446

http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/


# Issue
Resnet50 is applied on a real life dataset (very much different from 'cats-and-dogs'
experimental dataset) as a baseline feature extraction model, and on top of that,
a few more dense layers being added on top of it.

code

```python
from tensorflow import keras
def modeltrans_resnet():
    model=keras.models.Sequential()
    model.add(res_net)
    # To generate predictions from the block of features, average over the spatial 7x7 spatial locations,
    model.add(keras.layers.GlobalAveragePooling2D())
   #------- add a few more layers----
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.5))
#     model.add(keras.layers.Dense(64, activation='relu'))
#     model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(1, activation='sigmoid'))
    return model
model3 = modeltrans_resnet()

model3.compile(
  optimizer=keras.optimizers.Adam(lr=1e-3),
  loss=keras.losses.BinaryCrossentropy(),
  metrics=METRICS)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')
mcp_save = ModelCheckpoint('roof_transfer_0211', save_best_only=True, monitor='val_loss', mode='min')
# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')

model3_history = model3.fit(
    train_ds,
    # These are not real epochs
    steps_per_epoch = resampled_steps_per_epoch,
    epochs=15,
    callbacks=[earlyStopping, mcp_save],
    validation_data=(val_ds))
```

The fitted model then was evaluated back on the **training** dataset [\*1]. Although when training, tf shows a loss as low
as 0.2, the evaluation process returns a loss of 1.7, much much higher than the training process result. 

>[\*1]: I took 10 batches from the training dataset, which is oversampled in the same way.

Such issue, as I searched online, has been mentioned more than one times by different tf/keras users.

See below references
[https://github.com/keras-team/keras/issues/6977] 2017

[https://stackoverflow.com/questions/51123198/strange-behaviour-of-the-loss-function-in-keras-model-with-pretrained-convoluti/51124511#51124511] 2018

[https://github.com/keras-team/keras/issues/9214] 2018

[https://github.com/keras-team/keras/issues/10014] 2018

[https://github.com/keras-team/keras/issues/10045] 2018

So it looks like a problem bothering people for quite some time.

# Potential explanation

## Difference between calculating on the whole dataset vs calculating on minibatches
When evaluating back on the same training dataset, we will expect the evaluation metric to be
**a little** different comparing with the metric shown in the training process. The reason is that
the training metric are not calculated on the whole dataset, but rather a batch average applied 
on each mini-batch metric. According to the keras official document, the evaluation metric, is usually
better than the training metric [https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss].




However, this case has an opposite performance (training loss << testing loss), which might not be well
explained by the above document.


## The weird action of keras BN (batch-normalization) layer

Another explanation could be that the problem is due to the batch normalization layer[\*2] (http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/; posted by https://github.com/datumbox)

>[\*2] A naive explanation of a bn layer is that it standardize the output of the previous layer
in a way similar to a z-standardization. This method is supposed to help network train faster
and more accurate







