
# Introduction

This is a post relating with an issue encoutered recently, in which
the model.fit and model.evalute gives very different performance
metrics even on the same dataset. Quite a few similar issues have been
posted online. I will briefly introduce the issue, its potential causes,
and possible solutions in the post.


# Reference

https://stackoverflow.com/questions/51123198/strange-behaviour-of-the-loss-function-in-keras-model-with-pretrained-convoluti/51124511#51124511


https://github.com/keras-team/keras/issues/6977

https://github.com/tensorflow/tensorflow/issues/36446

https://github.com/keras-team/keras/pull/9965

https://github.com/keras-team/keras/issues/10014

https://github.com/keras-team/keras/issues/10045

https://github.com/tensorflow/tensorflow/issues/36446

http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/


# Issue
Resnet50 is applied on a real life dataset (very much different from 'cats-and-dogs'
experimental dataset) as a baseline feature extraction model, and on top of that,
a few more dense layers being added on top of it.

code

```python
from tensorflow import keras
def modeltrans_resnet():
    model=keras.models.Sequential()
    model.add(res_net)
    # To generate predictions from the block of features, average over the spatial 7x7 spatial locations,
    model.add(keras.layers.GlobalAveragePooling2D())
   #------- add a few more layers----
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.5))
#     model.add(keras.layers.Dense(64, activation='relu'))
#     model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(1, activation='sigmoid'))
    return model
model3 = modeltrans_resnet()

model3.compile(
  optimizer=keras.optimizers.Adam(lr=1e-3),
  loss=keras.losses.BinaryCrossentropy(),
  metrics=METRICS)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')
mcp_save = ModelCheckpoint('roof_transfer_0211', save_best_only=True, monitor='val_loss', mode='min')
# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')

model3_history = model3.fit(
    train_ds,
    # These are not real epochs
    steps_per_epoch = resampled_steps_per_epoch,
    epochs=15,
    callbacks=[earlyStopping, mcp_save],
    validation_data=(val_ds))
```

The fitted model then was evaluated back on the **training** dataset [\*1]. Although when training, tf shows a loss as low
as 0.2, the evaluation process returns a loss of 1.7, much much higher than the training process result. 

>[\*1]: I took 10 batches from the training dataset, which is oversampled in the same way.

Such issue, as I searched online, has been mentioned more than one times by different tf/keras users.

See below references
[https://github.com/keras-team/keras/issues/6977] 2017

[https://stackoverflow.com/questions/51123198/strange-behaviour-of-the-loss-function-in-keras-model-with-pretrained-convoluti/51124511#51124511] 2018

[https://github.com/keras-team/keras/issues/9214] 2018

[https://github.com/keras-team/keras/issues/10014] 2018

[https://github.com/keras-team/keras/issues/10045] 2018

So it looks like a problem bothering people for quite some time.

# Potential explanation

## Difference between calculating on the whole dataset vs calculating on minibatches
When evaluating back on the same training dataset, we will expect the evaluation metric to be
**a little** different comparing with the metric shown in the training process. The reason is that
the training metric are not calculated on the whole dataset, but rather a batch average applied 
on each mini-batch metric. According to the keras official document, the evaluation metric, is usually
better than the training metric [https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss].




However, this case has an opposite performance (training loss << testing loss), which might not be well
explained by the above document.


## The weird action of keras BN (batch-normalization) layer

Another explanation could be that the problem is due to the batch normalization layer[\*2] (http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/; posted by https://github.com/datumbox)

>[\*2] A naive explanation of a bn layer is that it standardize the output of the previous layer
in a way similar to a z-standardization. This method is supposed to help network train faster
and more accurate

A **short** explanation for this is that, the **BN layers act differently** in train process (.fit) vs inference process
(.evaluate / .predict). While in **training**, the mean and std calculated were from the **mini batches used for training**,
the inference process used the **pre-trained** parameters. Such acts keep the same, even if you set **layer.trainable=True**
in transfer learnings.

A more detailed explanation:

Such behaviour should cause no problem (and it should be so), if the model was trained & tested on the same
dataset. The problem is with **transfer learning**.

In transfer learning, the new dataset could be very different from the old data used to train the pretrained model (renet,
vgg16, etc.). So the mean and std can also be very different. When you train and evaluate with different mean and 
std, you are actually using different weights, and in that case shouldn't expect consistant outcomes.

In some people's opinion (myself included), it looks to be better to either keep the BN parameters **completely frozen** in
transfer learning (i.e., keep using the stats from pretrained model dataset for both training and evaluation), or use 
the **mini-batch stats for evaluation**, if you have done so in training. To me, using different parameters in training and
testing just seems to be **wrong**, even though such actions might help with generalization.

Here are the post and patch proposed by https://github.com/datumbox. Though keras didn't accpet the patch (:P; I
will need to look more into the keras replay, though till now datumbox's explanation looks very reasonable
to me)


http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/

https://github.com/keras-team/keras/pull/9965


# possible solutions

1. https://github.com/datumbox posted a patch which completely frozen the BN layers if trainable is set to False
(looks like currently in keras, only the value of learning_phase determines whether this layers parameters
are block or not).[http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/]

2.  In [https://stackoverflow.com/questions/51123198/strange-behaviour-of-the-loss-function-in-keras-model-with-pretrained-convoluti/51124511#51124511], [Andrey Kite Gorin](https://stackoverflow.com/users/7907205/andrey-kite-gorin) proposed two 
methods, either set all BN layers to be **trainable** for your transfer learning, or use **model_base.predict() to extract
your feature** instead of puting it in one tf.graph.

3. Use a base model without bn layers (for example, vgg16), instead of using resnet50.

# Self-experiment

## method 1 - to be tried

method 1 requires chaning the underlying keras code, which would be too risky to do on official projects.


## method 2 - make bn layers trainable

It is doable, I've made changes to the code, and it needs to be tested.


## method 3 - extract the feature then build the model

tbd - needs more model modification


## method 4 - use vgg16

A small test has been done on trainig 2 epoches and test on 10 minibatches. And it looks
like the issue metioned above vanished.










