
# Introduction

This is a post relating with an issue encoutered recently, in which
the model.fit and model.evalute gives very different performance
metrics even on the same dataset. Quite a few similar issues have been
posted online. I will briefly introduce the issue, its potential causes,
and possible solutions in the post.


# Reference

https://stackoverflow.com/questions/51123198/strange-behaviour-of-the-loss-function-in-keras-model-with-pretrained-convoluti/51124511#51124511


https://github.com/keras-team/keras/issues/6977

https://github.com/tensorflow/tensorflow/issues/36446

https://github.com/keras-team/keras/pull/9965

https://github.com/keras-team/keras/issues/10014

https://github.com/keras-team/keras/issues/10045

https://github.com/tensorflow/tensorflow/issues/36446

http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/


# Issue
Resnet50 is applied on a real life dataset (very much different from 'cats-and-dogs'
experimental dataset) as a baseline feature extraction model, and on top of that,
a few more dense layers being added on top of it.

code

```python
from tensorflow import keras
def modeltrans_resnet():
    model=keras.models.Sequential()
    model.add(res_net)
    # To generate predictions from the block of features, average over the spatial 7x7 spatial locations,
    model.add(keras.layers.GlobalAveragePooling2D())
   #------- add a few more layers----
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.5))
#     model.add(keras.layers.Dense(64, activation='relu'))
#     model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(1, activation='sigmoid'))
    return model
model3 = modeltrans_resnet()

model3.compile(
  optimizer=keras.optimizers.Adam(lr=1e-3),
  loss=keras.losses.BinaryCrossentropy(),
  metrics=METRICS)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')
mcp_save = ModelCheckpoint('roof_transfer_0211', save_best_only=True, monitor='val_loss', mode='min')
# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')

model3_history = model3.fit(
    train_ds,
    # These are not real epochs
    steps_per_epoch = resampled_steps_per_epoch,
    epochs=15,
    callbacks=[earlyStopping, mcp_save],
    validation_data=(val_ds))
```

The fitted model then was evaluated back on the **training** dataset [^1].

>[^1]: I took 10 batches from the training dataset, which is oversampled in the same way.
